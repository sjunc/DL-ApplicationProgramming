
# 딥러닝 응용 프로그래밍
----  
기본 개념 복습  
인공지능 EX.전문가 시스템  
  |  
머신러닝 EX.인공신경망(ANN), SVM, DecisionTree  
  |  
딥러닝 EX.CNN(Convolutional Neural Network, RNN(Recurrent Neural Network), RBM(Restricted Boltzmann Machine)    

## 2주차 인공지능(AI, Artificial Intelligence)  
 정의  
- 인간의 지능적인 작업을 컴퓨터 시스템이 수행하도록 하는 기술과 학문 분야  
- 문제 해결, 추론, 학습, 자연어 처리, 로봇 제어 등이 포함됨  
 예  
- 챗봇 (예: ChatGPT) - 자율 주행 자동차  
- 음성 인식 (예: STT(Speech-to-Text) 시스템)  
- 가상 비서(Virtual Assistant) (예: Siri, Google Assistant)
    
### 머신러닝(ML, Machine Learning)  
정의  
- 인공지능의 하위 분야로, 컴퓨터가 명시적으로 프로그래밍되지 않고도 데이터를 학습하여 예측하거나 결정을 내리는 기술   
- 지도학습, 비지도학습, 강화학습으로 구분됨   
예  
- 이메일 스팸 필터링 (스팸/정상 메일 분류) classfication  
- 추천 시스템 (예: 넷플릭스 영화 추천)  
- 질병 진단 모델 (의료 데이터 분석)  
    
=컴퓨터가 학습할 수 있게 하는 알고리즘과 기술을 개발하는 분야  
  
• 머신 러닝 학습 과정  
• 머신 러닝은 학습 단계(learning)와 예측 단계(prediction)로 이루어짐  
• 학습 단계에서는 학습 데이터를 대상으로 머신 러닝 알고리즘을 적용하여 학습시키고, 이 학습 결과로 모형(model)이 생성됨  
• 예측 단계에서는 학습 단계에서 생성된 모형에 새로운 데이터를 적용하여 결과를 예측함  
   
과정   
특성 추출, 특징 추출(feature extraction)   
• 컴퓨터가 입력받은 데이터를 분석하여 일정한 패턴이나 규칙을 찾아 내려면 사람이 인지하는 데이터를 컴퓨터가 인지할 수 있는 데이터로 변환해 주어야 함  
• 이때 데이터별로 어떤 특징을 가지고 있는지 찾아내고, 그것을 토대로 데이터를 벡터로 변환하는 작업을 특성 추출이라고 함   
  
머신 러닝의 주요 구성 요소는 데이터와 모델(모형)   
데이터(자료)는 머신 러닝이 학습 모델을 만드는 데 사용하는 것  
편향되지 않는 훈련 데이터를 확보하는 것이 중요함  
또한, 학습에 필요한 데이터가 수집되었다면 훈련을 위해 ‘훈련 데이터셋’과 ‘검증 데이터셋’ 용도로 분리해서 사용  
보통 데이터의 80%는 훈련용으로, 20%는 검증용으로 분리해서 사용  

모델(model)은 머신 러닝의 학습 단계에서 얻은 최종 결과물로 가설이라고도 함  
• 예를 들어 “입력 데이터의 패턴은 A와 같다.”라는 가정을 머신 러닝에서는 모델이라고 함  
• 모델의 학습 절차는 다음과 같음  
1. 모델(또는 가설) 선택    
2. 모델 학습 및 평가   
3. 평가를 바탕으로 모델 업데이트(개선)  
    
• 이 세 단계를 반복하면서 주어진 문제를 가장 잘 풀 수 있는 모델을 찾음  
  
머신러닝 지도학습 / 비지도 학습 / 강화 학습 ->  
### 딥러닝(DL, Deep Learning)   
정의  
- 머신러닝의 하위 분야로, 인간의 뇌 신경망을 모방한 인공신경망(ANN)을 사용하여 데이터를 학습하는 기술  
- 다층 신경망(Deep Neural Network)을 활용하여 복잡한 패턴을 분석하고 처리  
예
- 이미지 인식(얼굴 인식, 자율 주행 자동차의 카메라)  
- 자연어 처리(번역, 감정 분석, 챗봇)   
- 음성 합성 (TTS, 딥페이크 음성)    

딥러닝은 인간의 신경망 원리를 모방한 심층 신경망(Multi Layer) 이론을 기반으로 고안된 머신 러닝 방법의 일종  
• 인간의 뇌가 많은 수의 뉴런(neuron)과 시냅스(synapse)로 구성되어 있는 것에 착안하여 컴퓨터에 뉴런과 시냅스 개념을 적용함  
• 각각의 뉴런은 복잡하게 연결된 수많은 뉴런을 병렬 연산하여 기존에 컴퓨터가 수행하지 못했던 음성·영상 인식 등의 처리를 가능하게 함  

인간의 뉴런 구조  
수상돌기: 주변이나 다른 뉴런에서 자극을 받아들이고, 이 자극들을 전기적 신호 형태로 세포체와 축색돌기로 보내는 역할을 함  
시냅스: 신경 세포들이 이루는 연결 부위로, 한 뉴런의 축색돌기와 다음 뉴런의 수상돌기가 만나는 부분  
축삭돌기: 다른 뉴런(수상돌기)에 신호를 전달하는 기능을 하는 뉴런의 한 부분  
- 뉴런에서 뻗어 있는 돌기 중 가장 길며, 단 한 개만 있음  
축삭말단: 전달된 전기 신호를 받아 신경 전달 물질을 시냅스 틈새로 분비  
  
이걸 본 따 퍼셉트론으로 만듬  
   
1. 데이터 준비: 딥러닝을 통해 구현하고자 하는 기능에 필요한 데이터들을 수집하고 전처리함.  
2. 모델 정의: 모델 정의 단계에서 모델의 구조를 결정하고, 신경망을 생성함.  
일반적으로 은닉층 개수가 많을수록(깊을수록) 성능이 좋아지지만 과적합이 발생할 확률이 높음  
3. 모델 컴파일: 컴파일 단계에서 활성화 함수, 손실 함수, 옵티마이저를 선택  
훈련 데이터셋 형태가 연속형이라면 평균 제곱 오차(Mean Squared Error, MSE)를 사용할 수 있으며, 이진 분류(binary classification)라면 크로스 엔트로피(cross-entropy)를 선택할 수 있음   
4. 모델 훈련: 훈련 단계에서는 한 번에 처리할 데이터양을 적절하게 지정함   
전체 훈련 데이터셋에서 일정한 묶음으로 나누어 처리할 수 있는 배치(Batch)와 훈련의 횟수인 에폭(Epoch) 선택이 중요함.  
이때 파라미터와 하이퍼파라미터에 대한 최적의 값을 찾을 수 있어야 함  
5. 모델 평가: 검증 데이터셋을 생성한 모델에 적용하여 실제로 예측을 진행해보는 단계.  
성능이 낮다면 파라미터를 튜닝하거나, 신경망 자체를 재설계할 수 있음

ex. 훈련 데이터셋 1000개 있음.   
batch size 20, epoch 10번임.  
한번 훈련데이터 전체를 볼 때까지 가중치가 업데이터가 50번 총 500번의 가중치 변경이 일어남.  iteration  
  
딥러닝 학습 과정에서 중요한 핵심 구성 요소는 신경망과 역전파  
딥러닝은 머신 러닝의 한 분야이기는 하지만, 심층 신경망(deep neural network)을 가지고 있다는 점에서 머신 러닝과 차이가 있음  
심층 신경망에는 데이터셋의 어떤 특성들이 중요한지 스스로에게 가르쳐 줄 수 있는 기능이 있음  

|특징 | 머신러닝 | 딥러닝 | 
|------|---|------|
|데이터 요구량| 적은 데이터로도 가능 | 대량의 데이터 필요 (수십만~수백만 개)|
|특징 추출 | 사람이 직접 특징을 설계 | 모델이 스스로 특징을 추출|
|필요 연산량 | 비교적 낮음 (CPU 가능) | 높음 (GPU/TPU 연산 필요)|
|모델 해석 가능성 | 해석 가능 (규칙 기반 예측) | 해석이 어려움 (블랙박스)| 
|학습 속도 | 상대적으로 빠름 (수분~수시간) | 느림 ( 수십 ~ 수백만 개 데이터, 몇 시간 ~ 며칠 )|
|적용 사례 | 의사 결정 트리, 랜덤 포레스트,SVM, K-NN, 선형 회귀 | CNN (이미지 인식), RNN (자연어 처리),GAN (이미지 생성),트랜스포머 (ChatGPT, BERT)|  
  
GPT (Generative Pretrained Transformer)  

### 지도학습 / 비지도학습 / 강화학습   
지도학습 : 입력 데이터 X -> 정답 데이터 Y   
정의  
입력 데이터(X)와 그에 대응하는 정답 데이터(Y, 레이블)가 함께 제공되는 학습방식   
- 모델은 이러한 데이터를 학습하여 새로운 입력에 대한 정확한 출력을 예측함   
 주요 기법  
- 분류(Classification): 데이터를 미리 정의된 카테고리로 분류함(환자인지, 강아지/고양이/스팸판별)  
- 회귀(Regression): 연속적인 값을 예측함(공부시간 별 성적)  
예  
- 이메일 스팸 필터링: 이메일의 내용이나 메타데이터를 기반으로 스팸 여부를 분류함   
- 손글씨 숫자 인식: 이미지에서 손으로 쓴 숫자를 인식하여 디지털 형식으로 변환함   
- 주택 가격 예측: 주택의 면적, 위치, 방 수 등의 특성을 기반으로 가격을 예측함   
- 음성 인식: 음성 데이터를 텍스트로 변환함   
   
비지도학습    
정의  
- 정답 데이터(레이블) 없이 입력 데이터(X)만으로 학습하는 방식  
- 데이터의 숨겨진 구조나 패턴을 발견하는 데 중점을 둠   
주요 기법  
- 군집화(Clustering): 유사한 특성을 가진 데이터들을 그룹으로 묶음  
- 차원 축소(Dimensionality Reduction): 고차원 데이터를 저차원으로 축소하여 주요 특징을 추출함.(PCA)    
예  
- 고객 세분화: 고객의 구매 행동이나 특성을 기반으로 유사한 그룹으로 분류함  
- 이상 탐지: 정상 패턴에서 벗어난 데이터를 탐지하여 이상 여부를 판단함   
   
강화학습
정의  
에이전트(Agent)가 환경(Environment)과 상호작용하며 보상(Reward)을 최대화하는 행동 전략을 학습하는 방식  
- 에이전트는 행동에 따른 보상을 통해 최적의 정책을 학습함(장기적인 보상을 고려해야 함)  
 주요 개념  
- 에이전트(Agent): 행동을 수행하는 주체  
- 환경(Environment): 에이전트가 상호작용하는 대상  
- 상태(State): 현재 환경의 상황  
- 행동(Action): 에이전트가 취할 수 있는 선택지  
- 보상(Reward): 행동의 결과로 얻는 피드백  
 예  
- 게임 AI: 체스, 바둑 등에서 최적의 수를 두기 위해 학습함  
- 로봇 제어: 로봇이 주어진 작업을 효율적으로 수행하도록 학습함  
- 자율 주행: 자동차가 도로 상황에 맞게 주행 전략을 학습함  
- 추천 시스템: 사용자의 반응을 기반으로 개인화된 콘텐츠를 추천함  

|구분 | 유형 | 알고리즘 | 
|------|---|------|
|지도학습(Supervised learning)|분류|K-최근접 이웃(K-Nearest Neighbor. KNN),서포트 벡터 머신(Support Vector Machine.SVM), 결정 트리(Decision tree),로지스틱 회귀 (logistic regression)|
|       | 회귀| 선형회귀(Linear Regression)| 
| 비지도학습(Unsupervised learning)|군집|K-평균 군집화 (K-means clustering) 밀도 기반 군집 분석 (DBSCAN) |
|    | 차원 축소| 주성분 분석|
|강화학습(Reinforcement learning)| -  |마르코프 결정 과정(Markov Decision Process, MDP)  |

### 사전 지식  
독립변수와 종속변수  
독립 변수 X -> 모델 -> 종속변수 Y  
독립변수(Independent Variable):     
 - 종속변수에 영향을 주는 변수   
 - 모델이 입력값으로 사용하는 변수  

종속변수(Dependent Variable):   
 - 다른 변수(독립변수)에 의해 영향을 받는 변수   
 - 예측하려는 목표 변수

예시
 마케팅 분석  
- 독립변수: 광고 비용, 프로모션 횟수  
- 종속변수: 매출액  
 부동산 가격 예측  
- 독립변수: 면적, 방 개수, 위치  
- 종속변수: 주택 가격  
 운동과 체중 감량  
- 독립변수: 운동 시간, 칼로리 섭취량  
- 종속변수: 체중 변화  
   
관계   
"독립변수(X)가 변하면 종속변수(Y)도 변한다."    
수식 예제:   
Y = aX + b   
- X: 독립변수     
- Y: 종속변수   
- a, b: 계수 a-기울기 b-절편   
   
평균(Mean, Average)과 분산(Variance)  
평균 (Mean)중앙값(Median)과 다름  
- 데이터 값들의 대표적인 중심값  
- 값을 모두 더한 후 개수로 나눈 값  
예시  
- 시험 점수: 70, 80, 90의 평균  
- (70 + 80 + 90) ÷ 3 = 80   
- 월간 판매량: (100, 120, 140) → 평균: 120개   
분산 (Variance)  
- 데이터가 평균을 기준으로 얼마나 퍼져 있는지 나타내는 값  
- 값이 클수록 데이터가 분산되어(흩어져) 있음  
핵심 요약  
평균: 데이터의 중심값  
분산: 데이터가 퍼진 정도  
 평균이 같아도 분산이 다를 수 있음!  
예:  
- (78, 80, 82) 평균 = 80, 분산 작음   
- (60, 80, 100) 평균 = 80, 분산 큼   
   
### 딥러닝 문제해결 과정  

1. 해결할 문제 정의 -> 2.데이터 수집 -> 3. 데이터 가공(데이터 전처리) -> 4. 딥러닝 모델 설계(모델구조 구현) 5. 딥러닝 모델 학습 -> 6. 성능 평가  

----
## 3주차 텐서플로 기초
### 벡터, 행렬, 텐서
• 인공지능(머신 러닝/딥러닝)에서 데이터는 벡터(vector)로 표현  
• 벡터는 [1.0, 1.1, 1.2]처럼 숫자들의 리스트로, 1차원 배열 형태  
• 행렬(matrix)은 행과 열로 표현되는 2차원 배열 형태  
• 텐서는 3차원 이상의 배열 형태  
  
#### 텐서플로(tensorflow)
데이터 흐름 그래프(data flow graph)를 사용하여 데이터의 수치 연산을 하는 오픈 소스 소프트웨어 프레임워크  
텐서플로에서 데이터의 수치 연산을 수행하기 위한 그래프   
그림과 같이 그래프의 노드(node)는 수학적 연산을 처리하고, 에지(edge)는 노드 사이의 관계를 표현하며, 데이터(텐서(tensor)) 이동을 수행   
  
텐서플로 아키텍처  
• 준비된 데이터를 사용하여 모델(모형)을 생성하고 저장할 수 있음  
• 생성된 모델을 사용하여 분류 및 예측으로 마무리할 수도 있지만, 텐서플로 허브(TensorFlow Hub)에 게시하여 재사용할 수도 있음 또한, 사용자에게 웹이나 모바일로 서비스를 배포할 수 있는 환경도 제공됨  
  
모델(모형)생성 -> 모델(모형) 저장 -> 모델(모형) 배포   
  
모델(모형) 생성:  
• 데이터 훈련을 위한 데이터셋과 모델을 생성하고 훈련할 수 있는 환경을 제공함  
• 모델의 학습 과정을 시각화하여 보여 줄 수 있는 텐서보드 같은 도구들을 제공  
• 모델(모형) 저장:  
• 텐서플로는 분산 환경에서 모델을 저장하고 배포할 수 있는 환경을 제공  
• 특히 웹이나 모바일 같은 다양한 환경에서 사용 가능하도록 호환성이 고려된 모델 저장소를 제공  
• 모델(모형) 배포:  
• 서버나 웹 환경에서 텐서플로를 사용하면 언어 및 플랫폼에 상관없이 모델을 쉽게 학습시키고 배포할 수 있음  
• 텐서플로 2.x에서는 포맷을 표준화하기 때문에 플랫폼과 컴포넌트 간 호환성도 확보할 수 있음  
텐서보드: epoch에 따른 accuracy 정확도 를 정리해준 표를 제공해줌.  

### 텐서플로의 주요 문법 model.complie() model.fit() model.evaluate() model.predict() 
모델 컴파일(model.compile) 과정에서의 주요 파라미터  
모델을 훈련하기 전에 필요한 파라미터들을 정의함  
주요 파라미터는 optimizer, loss function, metrics임.  
- 옵티마이저(optimizer): 데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정함(Adam, Nadam....)  
- 손실 함수(loss function): 훈련하는 동안 출력과 실제 값(정답) 사이의 오차를 측정함   
wx+b를 계산한 값(모델의 예측)과 y(정답)의 오차를 구해서 모델 정확성을 측정함 (예) MSE 등   
- 지표(metrics): 훈련과 검증 단계를 모니터링하여 모델의 성능을 측정(Accuracy[ 0 ~ 1 ]...)  
  
model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])  
ⓐ 아담(Adam) 옵티마이저를 사용  
ⓑ sparse_categorical_crossentropy는 다중 분류에서 사용되는 손실 함수  
ⓒ 'accuracy'는 훈련에 대한 정확도를 나타내는 것으로, 값이 1에 가까울수록 좋은 모델  
   
모델 훈련(model.fit) 과정  
• 앞서 만들어 둔 데이터로 모형을 학습(학습용/검증용 데이터)   
• 학습을 시킨다는 것은 y=wx+b라는 매핑 함수에서 w와 b의 적절한 값을 찾는다는 의미   
• w와 b에 임의의 값을 적용하여 시작하며, 모델에 데이터를 입력하면서 오차를 구하게 됨   
• 이때 오차가 줄어드는 방향으로 파라미터를 수정  

model.fit(x_train, y_train, epochs=10, batch_size=100, validation_data=(x_test, y_test), verbose =2)  
ⓐ 입력 데이터  
ⓑ 정답(label) 데이터  
ⓒ 학습 데이터를 학습하는 횟수  
ⓓ 한 번에 학습하는 데이터의 개수  
ⓔ 검증 데이터(validation data)  
각 epoch마다 검증 데이터의 정확도도 함께 출력하게 할 수 있음. epoch 마다 validation으로 평가  
검증 데이터로 확인된 정확도는 훈련이 잘 되고 있는지 보여주는 지표로 사용할 수 있으며, 검증 데이터를 모델이 보면서 학습하지는 않음  
ⓕ verbose 학습 진행 상황을 보여 줄지 옵션을 지정함  
    
모델 평가(model.evaluate) 과정  
• 주어진 검증 데이터셋을 사용하여 모델을 평가함   
• 평가가 끝나면 검증 데이터셋에 대한 손실 값과 정확도가 결과로 표시됨   
    
model.evaluate(x_test, y_test, batch_size=32)  
ⓐ 검증 데이터셋  
ⓑ 결과(label) 데이터셋  
ⓒ 한 번에 학습할 때 사용하는 데이터 개수  

모델 사용(model.predict) 과정  
• 훈련된 모델을 사용하여 다음 예시 코드처럼 실제 예측을 진행함  
model.predict(y_test)  

### 실습 환경세팅
anaconda prompt 가상환경 dl 생성  
conda create -n dl  
conda activate dl   
pip install tensorflow  

모델 정의할 때  

Sequential API  
간결하며 대부분의 문제에 적합하다.  
model = tf.keras.sequential()  
model.add(Dense(4, activation = '', input_shape = (4,), weights = (, ), name = ""))  
단순히 여러 개 쌓는 형태, 복잡한 모델 생성 한계  

Functional API  
복잡한 모델 생성에서 한계를 극복  
다중 입력과 다중 출력 정의 가능  
input_layer = Input(shape=(X.shape[1],)) 입력  
dense_layer_1 = Dense(15, activation = 'relu')(input_layer) 은닉1  
dense_layer_2 = Dense(10, activation = 'relu')(dense_layer_1) 은닉2  
output = Dense(y.shape[1], activation = 'softmax')(dense_layer_2) 출력  
model = Model(inputs = input_layer, outputs = output)  
입력 데이터 형태를 input() 파라미터로 사용해 입력층 정의  
이전 층을 다음 층의 입력으로 사용  
model()에 입력과 출력 정의  

## 머신러닝 핵심 알고리즘  
부지런한 학습자와 게으른 학습자  
부지런한 학습자(eager learner)  
데이터셋 내부의 관계를 일반화하고 이를 이용해 처음 보는 데이터의 결과를 예측  
ex) 의사결정나무, 규칙 유도  
주어진 데이터에 기초하여 식 y=f(X)와 같은 수학적 관계를 개발한 후, 새로운 데이터에 대한 결과를 예측   
입력변수와 타겟변수 간의 실제 관계를 가장 잘 근사시키려 함(타겟변수-정답, 실제관계-가설)   
게으른 학습자(lazy learner)  
간단한 방법  
학습용 데이터셋에서 비슷한 레코드를 검색해서 예측함  
학습용 데이터셋을 학습이 아닌 검색(lookup)용 테이블로 사용해서 입력변수들과 일치하는 레코드를 찾아 타겟변수를 예측함  
비슷한 레코드들은 n차원 공간에서 가까이 위치하고, 타겟 변수의 클래스가 같음을 이용함  
ex) k-NN 알고리즘의 기본적인 원리  

### k-NN(k-Nearest Neighbour) k-최근접 이웃
대표적인 lazy learner 기반 알고리즘  
학습용 데이터셋 전체를 ‘기억’한 후, 클래스를 모르는 레코드를 분류할 때 이 레코드의 입력변수들을 학습용 데이터 전체와 비교해서 가장 가까운 것을 찾음  
가장 가까운 학습용 레코드의 클래스 레이블이 시험용 레코드의 예측하고자 하는 클래스 레이블   
비모수적 (nonparametric) 방법으로, 일반화나 데이터셋의 분포를 찾으려는 시도를 하지 않음  
!)  
모수적 방법(parametric method): 관측 값이 어느 특정한 확률분포를 따른다고 전제한 후 그 분포의 모수(parameter)에 대한 검정을 실시하는 방법(가설을 세우고 그에 근사하게 다가가는 방식)   
비모수적 방법(nonparametric method): 관측 값이 어느 특정한 확률분포를 따른다고 전제할 수 없거나 또는 모집단에 대한 아무런 정보가 없는 경우에 실시하는 검정방법.(데이터 형태나 모양을 보고 유연하게 모델링)  
모수에 대한 언급이 없으며 분포무관 방법.   
  
일단 학습용 레코드를 기억하고, 시험용 레코드와 가장 가까운 학습용 레코드를 찾으면 됨   
데이터셋에 있는 어떤 레코드라도 n차원 공간에서 하나의 점으로 나타낼 수 있음  
n: 속성의 개수  
3차원을 초과하는 경우에는 시각화하기 어렵지만, 2차원 공간에서 할 수 있는 연산들은 n차원 공간에서 수행 가능함  

iris 붓꽃 데이터   

k는 클래스를 모르는 시험용 레코드를 예측할 대 고려해야 할 최근접 학습용 레코드의 개수
- k=1일 경우   
가장 가까운 레코드 하나를 찾아 그 레이블을 예측치로 간주함  
최근접한 학습용 레코드가 특이값이거나 클래스가 틀리게 입력된다면....
k=1) 단순하지만 특이값에 약하다.     
- k=3일 경우  
가까운 레코드 3개를 고려함   
그 레코드 중 과반수의 레코드의 레이블을 클래스로 예측함   
그 레코드 중 과반수 이상인 레이블로 클래스를 예측함   
타겟 레코드의 클래스를 투표로 결정하므로 2-클래스 문제일 경우 k값은 주로 홀수로 정함  
k>1) 다수결로 안정성이 높으나 k가 너무 크면 경계가 모호해짐  
  
### 근접성 척도  
k-NN 알고리즘의 유효성은 시험용 레코드를 기억된 학습용 레코드와 비교할 때 얼마나 비슷한가를 결정하는 것에 전적으로 달려 있음  
근접성 척도 : 두 레코드 사이의 유사도를 수량화함  
- 거리  
유클리드 거리, 맨해튼 척도, 쳬비셰프 척도  
- 상관관계 유사도
- 단순 매칭 계수
- 자카드 유사도
- 코사인 유사도

유클리드 거리  
2차원 공간에서 두 점 X(x1 , x2 )와 Y(	y1 ,y2 ) 사이의 거리  
Dinstance d = 루트[ (x1 - y1)** + (x2 - y2)** ]
n차원 공간으로의 일반화   

ex) 4차원 아이리스 데이터셋의 데이터 포인트 X=(4.9, 3.0, 1.4, 0.2), Y=(4.6, 3.1, 1.5, 0.2) 거리는?   
루트 0.3** + (-0.1)** + (-0.1)** + 0**      -> 루트 0.11   
   
신용점수와 연간수입 데이터  
pair A : X=(500, $40,000), Y=(600, $40,000)  
pair B : X=(500, $40,000), Y=(500, $39,800)  

신용점수는 수치는 100이지만 20% 줄었고 연간수입은 200이 줄었지만 20만원 수준임(단위가 다름)  
문제점  
각 속성들의 크기와 측정단위가 통일되지 않으면 통일이 필요함   
각 레코드 사이의 공정한 비교를 위해 데이터셋의 모든 속성은 측정 대상과 측정 단위에 관해 동질적(homogenous)일 필요가 있음   
문제 완화를 위해 입력값들을 정규화(normalization)하는 작업이 필요함    
   
정규화 방법  
범위 변환(Min-Max Nomalization) : 모든 속성들의 값을 특정 최소값, 최대값(예.0~1 사이로) 사이의 크기로 지정함   
Z-변환 (z-score Nomalization) : 각 값에서 평균을 뺀 후 표준편차로 나눔  
변환된 값들의 평균이 0이고, 표준편차가 1인 분포를 갖게 됨  
   
Z-변환  (값 - 평균)/표준편차 값들의 평균이 0, 표준편차 1   
꽃받침의 길이를 변환하면?  
최솟값: 4.3, 최댓값 : 7.9, 평균: 5.84, 표준편차: 0.83  
4.3과 7.9 사이의 값을 가졌지만 변환 후에는 변환 전 표준편차는 0.83이지만, 변환 후 1이 됨  

맨해튼(Manhattan) 거리  
빌딩이 늘어져있고 체크무늬의 거리임. 개별 속성 사이의 차이의 합  
X(1,2), Y(3,1)   
맨해튼 거리는   
x와 y x값의 차이2, y값 차이 1 | 2 + 1 = 3   

쳬비셰프(Chevyshev) 거리  
데이터셋에 있는 모든 속성들의 사이의 차이 중 최댓값  
X(1,2), Y(3,1)  
쳬비셰프 거리는  
x와 y x값의 차이2, y값 차이 1 |  가장 큰 값의 차이는 2    
X(1, 2, 3) Y(10, 20, 30) 이면 27  
  
민코프스키(Minkowski) 거리 척도로 일반화 가능  
p=1 : 맨해튼 거리  
p=2 : 유클리드 거리  
p=∞ : 쳬비셰프 거리  
   
- 척도 선택은 데이터 유형에 따라 함  
수치형 – 유클리드  
이진형 – 맨해튼, 해밍 거리  

k개의 최근접 이웃이 결정되면, k개 중 다수인 클래스가 예측 클래스임  
y’= maximum class(y1,y2, ''', yk)   
y’ : 시험용 데이터 포인트의 타겟 클래스에 대한 예측치  
yi : i번째로 가까운 이웃의 클래스  

가중치
k가 1보다 클 때, 최근접 이웃들은 타겟 클래스의 예측에 있어서 먼 이웃보다 발언권이 더 있어야 한다고 주장할 수 있음  
먼 이웃일수록 최종 클래스를 결정할 때 미치는 영향 감소  
모든 이웃들에게 가중치를 할당함으로써 반영할 수 있음 (**가까울수록 큰 가중치**)  
가중치는 예측 클래스를 계산하는 마지막 다중 투표 단계에 포함시킴  
가중치의 두 가지 조건  
이웃들로부터 주어진 데이터 포인트까지의 거리에 반비례함  
모든 가중치의 합이 1임  
가중치  
속성이 수치형일 때 잘 활용됨  
다른 유형의 속성을 수치형으로 변환해야 범주형으로 사용하는 경우보다 더 많은 정보를 얻을 수 있음  
속성이 범주형이면 두 점이 같으면 거리가 0, 두 점이 다르면 거리가 1임  
d(흐림, 맑음)=1, d(맑음, 맑음)=0  
속성이 순서형이고, 값이 3개 이상이면 순서형의 값을 0,1,2... 정수형으로 변환하여 수치형으로 다룰 수 있음  
수치형으로 변환하지 않고 거리를 구하면 같으면 0, 다르면 1 이렇게만 나타남  
## 4주차 머신러닝 알고리즘 
근접성 척도: 상관관계  
데이터 포인트 X와 Y 사이의 상관계수 : 둘 사이의 ‘선형‘ 관계에 대한 척도  
Pearson 상관계수  
-1~1 사이의 값  
-1이면 완전한 음의 상관관계, 1이면 완전한 양의 상관관계  
0이면 ‘선형’ 관계가 없음 (비선형 관계가 있을 수 있음)  
  *값의 차이보다 방향성과 경향이 중요한 데이터를 비교할 때   
2개의 데이터 포인트 X와 Y 사이의 상관계수  
Sxy : X와 Y의 공분산(covariance) Sx : X의 표준편차  
  
근접성 척도: 단순 매칭 계수
단순 매칭 계수(Simple Matching Coefficient, SMC)  
데이터셋이 이진형 속성을 가질 때 사용함    
전체 발생 횟수에서 0이나 1이 동시에 발생하는 횟수의 비율임  
  
근접성 척도: 자카드 유사도  
자카드 유사도(Jaccard similarity)  
X와 Y가 텍스트 문서라면?  
각 단어는 문서 행렬(document matrix), 또는 문서 벡터(document vector)라고 불리는 데이터셋에서 하나의 속성이 됨  
속성의 개수가 매우 많은 수천개의 속성을 다루는 경우가 발생  
두 문서 X, Y 비교 시 속성값의 대부분은 0이 됨  
= 두 문서가 같은 단어를 갖는 경우가 매우 드물다  
단어의 발생(occurrence)과 미발생(nonoccurrence)을 비교하면 큰 의미가 있는 정보를 얻기 어려움  
단순 매칭 유사도 척도에 ‘미발생‘ 횟수를 제외하고, 발생 횟수에 대해서만 유사도를 측정  

근접성 척도: 코사인 유사도
문서 벡터의 속성은 단어의 발생/미발생을 나타냈음  
단어의 존재/부존재가 아닌 발생 횟수를 이용해 더 많은 정보를 담은 벡터를 만들 수 있음  
  
결론  
게으른 학습자 모델은 데이터 마이닝 방법 중 가장 간단함  
핵심 기능은 학습용 데이터셋을 참조하거나 검색하는 것  
k-nn 모델에서는 크거나 작은 측정 단위를 가지는 속성이 유발하는 편향(bias)을 피하기 위해 정규화가 필요함  
시험용 레코드에 결측치가 있어도 매우 견고한(robust) 모델  
게으른 학습자 모델은 입력과 출력 사이의 관계를 설명할 수 없음(고차원일 때)  
k-nn 모델 구축은 ‘기억하기’이기 때문에 시간이 적게 소요됨  
클래스를 모르는 레코드를 분류하려면 시험용 레코드와 모든 학습용 레코드 사이의 거리를 계산해야 하여 학습용 데이터의 크기나 속성의 개수가 많으면 고비용이 들 수 있음  
k-nn 모델이 입출력 관계를 일반화하는 데에는 좋지 않지만, 학습용 레코드 사이에 존재하는 관계를 확장하는 데에 매우 효과적임  
순서형 값들을 정수값들로 변환하여 거리를 구할 수 있음  
양질의 분석 결과를 내기 위해 입력 속성들의 가능한 모든 값들이 포함된 학습용 레코드가 상당히 많아야 함  

### svm 
서포트 벡터 머신(SVM)  
패턴 인식, 텍스트 마이닝 등 많은 분야에서 좋은 성능을 보임  
컴퓨터과학, 통계학 수학적 최적화 이론으로부터 주요 개념들이 도출됨  
개념과 용어  
같은 클래스에 속하는 데이터 포인트들끼리 모여있도록 경계를 설정하는 분류 기법  
학습용 데이터셋에 대해 일단 경계를 정하면, 새로운 데이터를 분류할 때 경계 내부인지,외부인지 점검함  
일단 경계가 확정되면 대부분의 학습용 데이터는 불필요함  
- 모든 학습용 데이터가 불필요하다(X)  
필요한 데이터 포인트들은 경계를 찾고 수정하는 데 관여하는 서포트 벡터(핵심 포인트)들 뿐
- 서포트벡터는 경계를 support 하는 데이터 포인트들이며, 경계를 찾고 수정하는데 관여함
각 데이터 포인트들은 벡터이다  
- 데이터 포인트 : 많은 수의 속성값을 포함하는 데이터로 구성된 하나의 행  
    
서포트 벡터 머신의 경계 : 초평면(hyperplane)  
속성이 2개인 경우 2차원에 데이터 포인트가 결정되고, 그 데이터 포인트들을 클래스별로 나눌 수 있는 것은 직선 또는 곡선임  
속성이 3개인 경우 3차원에 데이터 포인트가 결정되고, 그 데이터 포인트들을 클래스별로 나눌 수 있는 것은 평면이거나, 복잡한 면이 있음  
고차원에서는 ‘초평면’이 경계에 대한 포괄적인 이름임  
서포트 벡터 머신의 경계 : 초평면(hyperplane)  
동일한 데이터셋에 대해 수많은 초평면이 있을 수 있음  
이 데이터 포인트들을 잘 나누는 가장 적절한 초평면을 고르는 것이 SVM의 목적  
클래스를 분리할 때 오류가 최소인 경계가 가장 좋은 경계  
두 구역 사이의 평균 거리(여백, margin)가 최대가 되도록 하는 경계가 가장 좋은 경계  
SVM 알고리즘은 본질적으로 여백을 최대화하는 최적화 기법을 수행함  
여백을 최대화하면 일반화 오차가 줄어듦  
서포트 벡터 : 경계의 구성에 영향을 주는 데이터 포인트들, 마진의 경계에 걸치는 샘플들  

항상 데이터가 깔끔하게 분리되기는 어렵다    
데이터가 선형 분리 가능(linearly separable)인 경우는 드물다  
선형분리가 되는 경우, 여백 안에 많은 점들이 존재하게 됨  
가장 좋은 초평면은 여백 안의 점들의 수가 최소가 되게 하는 초평면임    
여백 내부의 모든 ‘오염된(co ntaminant)’ 점들에 벌점(penalty)을 부과한 후, 총 벌점이최소인 초평면을 선택함  

선형 분리가 불가능한 경우 = 클래스 사이를 분리하는 평면, 직선이 없는 경우  
ex) 초평면(타원이나 원 등)이 클래스를 쉽게 분리할 수 있는 경우   
커널 함수(kernel function)를 이용해 선형분리가 가능한 형태로 변환  
커널 함수   
비선형 공간을 선형으로 변환하는 옵션 제공  
다양한 커널 함수들(다항식, 시그모이드 함수.. 등)  
사용자는 소프트웨어에서 적당한 커널 함수를 선택하면 됨(주로 다항식과 반지름 기반 함수를
사용함)  

선형 분리가 불가능한 경우에 분리할 수 있는 방법
두 변수 x,y로 이루어진 공간((x,y) 차원)을 x와 새로운 변수 z로 이루어진 공간 ((x,z) 차원)으로 변환시킴  
ex) z는 원의 방정식을 사용하여 변환하였음 -> 반지름에 대응하는 값이 나타남  
  
선형 분리가 불가능한 경우에 분리할 수 있는 방법  
변환된 공간에서 분류한 후, 다시 원래의 공간으로 역변환시킴  
  
핵심 단계들  
단계1 : 각 클래스의 경계를 찾음  
단계2 : 나누는 기준으로서 지정이 가능한 초평면들 중 각 경계까지의 거리를 최대로 하는
최선의 초평면 H를 찾음  
단계3 : 주어진 시험용 레코드를 분류하기 위해 초평면의 어느 쪽에 위치하는지 계산하여
판단함  

단계1 : 각 클래스의 경계를 찾음  
한 클래스에 해당하는 점들을 모두 연결하고, 그것의 외곽선인 convex hull(컨벡스 헐):[실제 계산 x)을 찾는다  
각 클래스는 자신의 convex hull을 갖고 있고, 클래스들이 선형 분리가 가능한 경우, convex
hull들은 서로 교차하지 않음  
  
단계2 : 최선의 초평면 H를 찾음  
가능한 초평면은 무한정 많음  
여백을 최대로 하는 초평면을 선택함  
수리최적화 이론 중 2차 계획법 문제로 여백을 최대로 하는 초평면을 찾음(수학적 결정)  
소프트웨어에서 제공하는 기능  
n개의 속성을 가진 데이터는 최소 n+1개의 서포트 벡터가 존재함  
서포트 벡터만 고려하여 경계를 결정하므로 수행 시간이 빠르다  
  
단계3 : 시험용 레코드를 분류함  
일단 경계와 초평면이 결정되면, 새로운 시험용 레코드를 분류할 때, 초평면의 어느 쪽에 놓여 있는지 계산함  
초평면의 수학식에 시험용 레코드 값을 대입하여 나온 계산값으로 클래스를 판별할 수 있음  
소프트웨어에서 제공하는 기능   
  
장점  
구축 후, 서포트 벡터가 변하지 않으면 소요 시간이 적음  
유용한 적용성(flexibility in application)  
이미지 프로세싱부터 사기 탐지, 텍스트 마이닝에 이르기까지 다양한 분야에 적용됨  
강건성(robustness)  
데이터의 변화가 작은 경우에는 리모델링하는 것이 불필요함  
반 과적합(overfitting resistance)  
데이터셋 내부의 클래스 경계는 소수의 서포트벡터만으로도 적절히 나타낼 수 있음  
단점  
속성의 수가 많으면(차원^) 어떤 커널 함수를 사용해야 할지 알기 어려우며, 2차식으로 시작하여 원하는 정확도 수준에 이를 때까지 점점 복잡한 함수를 이용해야 하므로 계산량이 많음  
고차원의 SVM은 계산량이 많아 구축시 시간 소요가 큼  
SVM은 학습시 모든 분류에 대해 dot product(내적) 연산을 하므로, 차원이 매우 높으면(=속성의 수가 많으면) 계산이 매우 느려짐  

### Decision Tree
• 결정 트리(Decision Tree, 의사결정 나무)  
• 결과 모델이 트리 구조이므로 결정 '트리'라고 함  
• 입력 데이터를 여러 단계의 질문(또는 조건)으로 나눠가면서  
각 단계마다 분기를 거치며, 결국 잎(leaf)에 도달하는 분석 방법  
• 장점: 해석이 쉬움, 시각화가 쉬움  
• 단점: 과적합 위험이 있음  
순도와 불순도   
• 결정 트리의 성능과 구조를 결정하는 핵심 기준  
• 순도(purity): 한 노드에 속한 데이터가 거의 모두 같은 클래스일 때, "순도가 높다"  
• 불순도(impurity): 한 노드에 여러 클래스가 섞여 있으면 "불순도가 높다"  
• 결정 트리는 학습 과정에서 불순도를 줄이는 방향으로 분기함  
• 결정 트리에서는 각 분기점에서 순도가 높아질수록 예측 정확도가 올라감  
• 불순도가 높은 노드는 더 분할해 순도를 높여야 하고, 순도가 충분히 높으면 그 부분을 잎(leaf) 노드로 결정함  
  
순도와 불순도 - 엔트로피, 지니계수  
• 순도가 증가하고 불확실성이 감소하는 것을 정보 이론에서는 정보 획득(information gain)이라고 하며, 불확실성을 계산하는 방법으로 엔트로피와 지니계수를 많이 사용함  
• 엔트로피(Entropy)  
: 확률 변수의 불확실성을 수치로 환산한 것  
엔트로피가 높을수록 불확실성이 높음.  
즉, 엔트로피 값이 0이면 불확실성이 최소 = 무질서 최소 = 순도 최대 = 같은 항목만 있음  
엔트로피 값이 1이면 불확실성이 최대 = 무질서 최대 = 순도 최소 = 각 항목이 같은 비율로 섞여 있음  
• 지니 계수(Gini Index)  
• 엔트로피 측정과 유사하지만 0과 0.5 사이임  
지니 계수 0 = 불순도 최소  
지니 계수 0.5 = 불순도 최대  














----
